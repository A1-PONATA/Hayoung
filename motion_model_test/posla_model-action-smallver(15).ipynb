{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 사용할 패키지 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers.normalization import BatchNormalization\n",
    "from tensorflow.python.keras.layers.convolutional import Conv2D\n",
    "from tensorflow.python.keras.layers.convolutional import MaxPooling2D\n",
    "from tensorflow.python.keras.layers.core import Activation\n",
    "from tensorflow.python.keras.layers.core import Flatten\n",
    "from tensorflow.python.keras.layers.core import Dropout\n",
    "from tensorflow.python.keras.layers.core import Dense\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.python.keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4622981600106460800\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 12161013025861999808\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 12648448\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 8254765847696499629\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2080, pci bus id: 0000:17:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6716833792\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 2427167009528639935\n",
      "physical_device_desc: \"device: 1, name: GeForce RTX 2080, pci bus id: 0000:b3:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 18335749225274995433\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 3)\n",
      "0.0 min 100\n",
      "0.0 min 200\n",
      "1.0 min 300\n",
      "2.0 min 400\n",
      "3.0 min 500\n",
      "5.0 min 600\n",
      "8.0 min 700\n",
      "10.0 min 800\n",
      "12.0 min 900\n",
      "15.0 min 1000\n",
      "18.0 min 1100\n",
      "22.0 min 1200\n",
      "25.0 min 1300\n",
      "29.0 min 1400\n",
      "33.0 min 1500\n",
      "39.0 min 1600\n",
      "43.0 min 1700\n",
      "48.0 min 1800\n",
      "54.0 min 1900\n",
      "60.0 min 2000\n",
      "66.0 min 2100\n",
      "72.0 min 2200\n",
      "78.0 min 2300\n",
      "85.0 min 2400\n",
      "92.0 min 2500\n",
      "100.0 min 2600\n",
      "107.0 min 2700\n",
      "115.0 min 2800\n",
      "123.0 min 2900\n",
      "132.0 min 3000\n",
      "140.0 min 3100\n",
      "149.0 min 3200\n",
      "159.0 min 3300\n",
      "168.0 min 3400\n",
      "178.0 min 3500\n",
      "188.0 min 3600\n",
      "198.0 min 3700\n",
      "209.0 min 3800\n",
      "220.0 min 3900\n",
      "231.0 min 4000\n",
      "243.0 min 4100\n",
      "254.0 min 4200\n",
      "266.0 min 4300\n",
      "279.0 min 4400\n",
      "292.0 min 4500\n",
      "304.0 min 4600\n",
      "318.0 min 4700\n",
      "331.0 min 4800\n",
      "345.0 min 4900\n",
      "359.0 min 5000\n",
      "373.0 min 5100\n",
      "388.0 min 5200\n",
      "403.0 min 5300\n",
      "418.0 min 5400\n",
      "433.0 min 5500\n",
      "449.0 min 5600\n",
      "465.0 min 5700\n",
      "481.0 min 5800\n",
      "498.0 min 5900\n",
      "514.0 min 6000\n",
      "531.0 min 6100\n",
      "549.0 min 6200\n",
      "567.0 min 6300\n",
      "585.0 min 6400\n",
      "603.0 min 6500\n",
      "621.0 min 6600\n",
      "640.0 min 6700\n",
      "659.0 min 6800\n",
      "679.0 min 6900\n",
      "698.0 min 7000\n",
      "718.0 min 7100\n",
      "738.0 min 7200\n",
      "759.0 min 7300\n",
      "780.0 min 7400\n",
      "801.0 min 7500\n",
      "822.0 min 7600\n",
      "844.0 min 7700\n",
      "866.0 min 7800\n",
      "888.0 min 7900\n",
      "910.0 min 8000\n",
      "933.0 min 8100\n",
      "956.0 min 8200\n",
      "980.0 min 8300\n",
      "1004.0 min 8400\n",
      "1029.0 min 8500\n",
      "1054.0 min 8600\n",
      "1079.0 min 8700\n",
      "1103.0 min 8800\n",
      "1128.0 min 8900\n",
      "1154.0 min 9000\n",
      "1179.0 min 9100\n",
      "1205.0 min 9200\n",
      "1232.0 min 9300\n",
      "1258.0 min 9400\n",
      "1286.0 min 9500\n",
      "1313.0 min 9600\n",
      "1341.0 min 9700\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train = np.empty((0, 320, 320, 3))\n",
    "y_train = np.empty((0, 3))\n",
    "print(y_train.shape)\n",
    "t= time.time()\n",
    "training_data = glob.glob('/home/pirl/Documents/selected_motion_data10000/*.npz')\n",
    "#print(training_data)\n",
    "idx=1\n",
    "for single_npz in training_data:\n",
    "    with np.load(single_npz) as data:\n",
    "        #print(data.files)\n",
    "        x = data['train']\n",
    "        y = data['training_labels']\n",
    "        #print(x.shape)\n",
    "    x = np.reshape(x, (-1, 320, 320,3))\n",
    "    \n",
    "    x_train = np.vstack((x_train, x))\n",
    "    y_train = np.vstack((y_train, y))\n",
    "    if idx % 100 ==0:\n",
    "        print(str((time.time()-t)//60)+\" min\",idx)\n",
    "    idx+=1\n",
    "print(x_train.shape)\n",
    "# train test split, 7:3\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.3, random_state = 42)\n",
    "\n",
    "y_train = y_train[:,:]\n",
    "y_test = y_test[:,:]\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "\n",
    "#y_data = pd.DataFrame(y_total[:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#left = y_data.loc[y_data[:][0] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#left.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#right = y_data.loc[y_data[:][1] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#right.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward = y_data.loc[y_data[:][2] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 25 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터 확인\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt_row = 5\n",
    "plt_col = 5\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "f, axarr = plt.subplots(plt_row, plt_col)\n",
    "\n",
    "for i in range(plt_row*plt_col):\n",
    "\n",
    "    sub_plt = axarr[int(i/plt_row), int(i%plt_col)]\n",
    "    sub_plt.axis('off')\n",
    "    sub_plt.imshow(x_train[i].reshape(64, 64,3))\n",
    "    \n",
    "    label = np.argmax(y_train[i])\n",
    "                      \n",
    "    if label == 2 :\n",
    "        direction = 'left'\n",
    "    elif label == 1:\n",
    "        direction = 'right'\n",
    "    elif label == 0:\n",
    "        direction = 'forward'\n",
    "#     elif label == 3:\n",
    "#         direction = 'backward'\n",
    "                      \n",
    "    sub_plt_title = str(label) + \" : \" + direction\n",
    "    sub_plt.set_title(sub_plt_title)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image size = 160 x 320\n",
    "def posla_net():\n",
    "    \n",
    "    # model setting\n",
    "    H = 320\n",
    "    W = 320\n",
    "    CH = 3\n",
    "\n",
    "    inputShape = (H, W, CH)\n",
    "\n",
    "    activation = 'relu'\n",
    "    keep_prob_conv = 0.25\n",
    "    keep_prob_dense = 0.5\n",
    "\n",
    "    #init = 'glorot_normal'\n",
    "    #init = 'he_normal'\n",
    "    init = 'he_uniform'\n",
    "    chanDim = -1\n",
    "    classes = 3\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # CONV => RELU => POOL\n",
    "    model.add(Conv2D(3, (3, 3), padding=\"valid\", input_shape=inputShape, kernel_initializer=init, activation=activation))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(9, (3, 3), padding=\"valid\", kernel_initializer=init, activation=activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(18, (3, 3), padding=\"valid\", kernel_initializer=init, activation=activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3), padding=\"valid\", kernel_initializer=init, activation=activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(80, kernel_initializer=init, activation=activation))\n",
    "    model.add(Dropout(keep_prob_dense))\n",
    "    \n",
    "    model.add(Dense(15, kernel_initializer=init, activation=activation))\n",
    "    model.add(Dropout(keep_prob_dense))\n",
    "    \n",
    "    # softmax classifier\n",
    "    model.add(Dense(classes , activation = 'softmax'))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 62, 62, 3)         84        \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 62, 62, 3)         12        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 31, 31, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 29, 29, 9)         252       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 14, 14, 9)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 12, 12, 18)        1476      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 6, 6, 18)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 4, 4, 32)          5216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 80)                10320     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 15)                1215      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 48        \n",
      "=================================================================\n",
      "Total params: 18,623\n",
      "Trainable params: 18,617\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = posla_net()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.utils import plot_model\n",
    "plot_model(model, to_file= 'video_net_plot.png', show_shapes = True, show_layer_names = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 모델 파라미터 셋팅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "적은 수의 이미지 데이터가 존재할 경우 데이터를 늘리는 용도로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,\n",
    "    height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
    "    horizontal_flip=True, fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "# EPOCHS = 50\n",
    "# INIT_LR = 1e-3\n",
    "# BS = 32\n",
    "# split_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "INIT_LR = 1e-4\n",
    "BS = 256\n",
    "split_ratio = 0.2\n",
    "\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "    metrics=[\"accuracy\"])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_binary = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7774 samples, validate on 1944 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 8370061312",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-34d4e901fd77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                  \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit_ratio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                  \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                  \u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreduce_lr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m                 )\n",
      "\u001b[0;32m~/anaconda3/envs/frcnn1/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1639\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/frcnn1/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/frcnn1/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2945\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`inputs` should be a list or tuple.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2947\u001b[0;31m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2948\u001b[0m     \u001b[0mfeed_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2949\u001b[0m     \u001b[0marray_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/frcnn1/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    463\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_SESSION\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m       \u001b[0m_SESSION\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_default_session_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_SESSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/frcnn1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m   1549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \"\"\"\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m     \u001b[0;31m# NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_graph_context_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/frcnn1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewSessionRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 8370061312"
     ]
    }
   ],
   "source": [
    "\n",
    "hist = model.fit(x_train, y_train, \n",
    "                 epochs=EPOCHS, batch_size=BS, \n",
    "                 validation_split=split_ratio, \n",
    "                 verbose = 1\n",
    "                 ,callbacks=[reduce_lr]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.title('model loss')\n",
    "plt.plot(hist.history['loss'], label=\"loss\")\n",
    "plt.plot(hist.history['val_loss'], label=\"val_loss\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.ylim((0,2))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('model accuracy')\n",
    "plt.plot(hist.history['acc'], label=\"acc\")\n",
    "plt.plot(hist.history['val_acc'], label=\"val_acc\")\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.ylim((0.4, 1))\n",
    "\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 모델 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#l_model = load_model('./model_data/VGG_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=BS)\n",
    "print('## evaluation loss and_metrics ##')\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat_idx = np.random.choice(x_test.shape[0], 10)\n",
    "xhat = x_test[xhat_idx]\n",
    "\n",
    "yhat_classes = model.predict_classes(xhat)\n",
    "\n",
    "for i in range(10):\n",
    "    print('True : ' + str(np.argmax(y_test[xhat_idx[i]])) + ', Predict : ' + str(yhat_classes[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 최종 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_total = np.vstack((x_train, x_test))\n",
    "y_total = np.vstack((y_train, y_test))\n",
    "\n",
    "print(x_total.shape)\n",
    "print(y_total.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "INIT_LR = 1e-4\n",
    "BS = 256\n",
    "split_ratio = 0.2\n",
    "\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hist = model.fit(x_total, y_total, \n",
    "                 epochs=EPOCHS, batch_size=BS, \n",
    "                 #validation_split=split_ratio, \n",
    "                 verbose = 1\n",
    "                 ,callbacks=[reduce_lr]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=BS)\n",
    "print('## evaluation loss and_metrics ##')\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat_idx = np.random.choice(x_test.shape[0], 10)\n",
    "xhat = x_test[xhat_idx]\n",
    "\n",
    "yhat_classes = model.predict_classes(xhat)\n",
    "\n",
    "for i in range(10):\n",
    "    print('True : ' + str(np.argmax(y_test[xhat_idx[i]])) + ', Predict : ' + str(yhat_classes[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import model_from_yaml\n",
    "\n",
    "model_yaml = model.to_yaml()\n",
    "with open(\"motion_model_demo.yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"motion_model_demo.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_28 (Conv2D)           (None, 62, 62, 3)         84        \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 62, 62, 3)         12        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 31, 31, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 29, 29, 9)         252       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 14, 14, 9)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 12, 12, 18)        1476      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 6, 6, 18)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 4, 4, 32)          5216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 80)                10320     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 15)                1215      \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 3)                 48        \n",
      "=================================================================\n",
      "Total params: 18,623\n",
      "Trainable params: 18,617\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
